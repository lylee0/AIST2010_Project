{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "songs = pd.read_csv(\"C:/Users/lly/Desktop/CUHK/Year 4/Sem 1/AIST2010/Project/Singers/dataset.csv\")\n",
    "\n",
    "audio_files_dir = (\"C:/Users/lly/Desktop/CUHK/Year 4/Sem 1/AIST2010/Project/Singers/dataset\")\n",
    "audio_files = os.listdir(audio_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 audio files\n",
    "audio_data = []\n",
    "for file in audio_files:\n",
    "    file_path = os.path.join(audio_files_dir, file)\n",
    "    data, sr = librosa.load(file_path, sr=22050)\n",
    "    data = librosa.to_mono(data)\n",
    "    data = librosa.util.normalize(data)\n",
    "\n",
    "    #num_segments = len(data) // sr\n",
    "    segments = np.array_split(data[:25 * sr], 25)\n",
    "\n",
    "    audio_data.append(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = []\n",
    "for i in audio_data:\n",
    "    final_dataset.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = np.array(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 22050)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc(data):\n",
    "    # Mel-frequency cepstral coefficients (MFCCs)\n",
    "    mfcc_audio = librosa.feature.mfcc(y=data, sr=22050, n_fft=512)\n",
    "    mfcc_audio = mfcc_audio.flatten()\n",
    "    return mfcc_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_audio_data = []\n",
    "for segment in final_dataset:\n",
    "    mfcc_audio = mfcc(segment)\n",
    "    mfcc_audio_data.append(mfcc_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_audio_data = np.array(mfcc_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 880)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_audio_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-197.73824  , -245.47433  , -244.586    , ...,   19.019123 ,\n",
       "          28.69128  ,   18.582964 ],\n",
       "       [-263.1695   , -346.9509   , -387.94293  , ...,  -11.384357 ,\n",
       "         -13.385475 ,   -6.5554695],\n",
       "       [-226.48969  , -251.4339   , -254.16122  , ...,  -19.222832 ,\n",
       "         -23.233484 ,   -3.8466933],\n",
       "       ...,\n",
       "       [-242.54521  , -254.78162  , -243.12556  , ...,   16.782604 ,\n",
       "          28.263494 ,   13.888154 ],\n",
       "       [-273.47614  , -323.56808  , -333.13507  , ...,   17.449774 ,\n",
       "          14.415916 ,   17.628853 ],\n",
       "       [-242.38847  , -274.5683   , -260.1299   , ...,   -3.8547637,\n",
       "           8.730096 ,    7.020806 ]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_audio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "mfcc_audio_data = scaler.fit_transform(mfcc_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7232258 ,  0.635     ,  0.64371514, ...,  0.73178315,\n",
       "         1.2846894 ,  1.5907325 ],\n",
       "       [ 0.19124846, -0.26539257, -0.64612335, ..., -0.9354239 ,\n",
       "        -1.0571544 , -0.9118112 ],\n",
       "       [ 0.4894672 ,  0.5821213 ,  0.5575631 , ..., -1.365255  ,\n",
       "        -1.6052599 , -0.6421512 ],\n",
       "       ...,\n",
       "       [ 0.35893056,  0.55241734,  0.6568552 , ...,  0.6091413 ,\n",
       "         1.2608804 ,  1.1233618 ],\n",
       "       [ 0.10745215, -0.05791895, -0.15299551, ...,  0.64572626,\n",
       "         0.49017298,  1.4957502 ],\n",
       "       [ 0.3602049 ,  0.37685183,  0.5038605 , ..., -0.5225307 ,\n",
       "         0.17372029,  0.43971384]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_audio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_df = pd.read_csv(\"C:/Users/lly/Desktop/CUHK/Year 4/Sem 1/AIST2010/Project/Singers/final_dataset_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label = np.array(final_dataset_df[final_dataset_df.columns[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Y_label_encoded = label_encoder.fit_transform(Y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(mfcc_audio_data, dtype=torch.float32)\n",
    "Y_label_encoded_tensor = torch.tensor(Y_label_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, Y_label_encoded_tensor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.unsqueeze(1)\n",
    "X_test = X_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * (input_size // 16), 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label_encoder.classes_)\n",
    "input_size = mfcc_audio_data.shape[1]\n",
    "model = CNNModel(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/70 [00:03<04:07,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/70], CNN Loss: 2.3033, CNN Accuracy: 9.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/70 [00:07<04:30,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/70], CNN Loss: 24.7642, CNN Accuracy: 10.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3/70 [00:11<04:24,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/70], CNN Loss: 3.3275, CNN Accuracy: 11.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/70 [00:16<04:28,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/70], CNN Loss: 2.4393, CNN Accuracy: 10.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5/70 [00:20<04:23,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/70], CNN Loss: 2.2995, CNN Accuracy: 11.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 6/70 [00:24<04:25,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/70], CNN Loss: 2.3090, CNN Accuracy: 10.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 7/70 [00:28<04:21,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/70], CNN Loss: 2.2759, CNN Accuracy: 16.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 8/70 [00:32<04:20,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/70], CNN Loss: 2.2297, CNN Accuracy: 18.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 9/70 [00:36<04:11,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/70], CNN Loss: 2.1935, CNN Accuracy: 16.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 10/70 [00:41<04:10,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/70], CNN Loss: 2.1375, CNN Accuracy: 21.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 11/70 [00:45<04:07,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/70], CNN Loss: 2.1054, CNN Accuracy: 27.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 12/70 [00:50<04:13,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/70], CNN Loss: 2.0314, CNN Accuracy: 26.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 13/70 [00:55<04:33,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/70], CNN Loss: 2.0197, CNN Accuracy: 29.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 14/70 [01:01<04:40,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/70], CNN Loss: 1.9112, CNN Accuracy: 32.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 15/70 [01:06<04:40,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/70], CNN Loss: 1.8409, CNN Accuracy: 34.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 16/70 [01:11<04:34,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/70], CNN Loss: 1.8132, CNN Accuracy: 36.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 17/70 [01:16<04:30,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/70], CNN Loss: 1.7181, CNN Accuracy: 39.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 18/70 [01:21<04:24,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/70], CNN Loss: 1.6562, CNN Accuracy: 41.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 19/70 [01:27<04:19,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/70], CNN Loss: 1.5910, CNN Accuracy: 44.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 20/70 [01:31<04:08,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/70], CNN Loss: 1.5605, CNN Accuracy: 44.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 21/70 [01:36<04:00,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/70], CNN Loss: 1.5085, CNN Accuracy: 46.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 22/70 [01:40<03:41,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/70], CNN Loss: 1.4894, CNN Accuracy: 46.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 23/70 [01:45<03:42,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/70], CNN Loss: 1.4017, CNN Accuracy: 50.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 24/70 [01:50<03:41,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/70], CNN Loss: 1.3444, CNN Accuracy: 52.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 25/70 [01:54<03:23,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/70], CNN Loss: 1.2846, CNN Accuracy: 53.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 26/70 [01:59<03:23,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/70], CNN Loss: 1.2122, CNN Accuracy: 57.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 27/70 [02:03<03:17,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/70], CNN Loss: 1.1535, CNN Accuracy: 58.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 28/70 [02:08<03:14,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/70], CNN Loss: 1.1211, CNN Accuracy: 61.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 29/70 [02:13<03:15,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/70], CNN Loss: 1.0428, CNN Accuracy: 63.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 30/70 [02:18<03:11,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/70], CNN Loss: 0.9732, CNN Accuracy: 66.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 31/70 [02:23<03:08,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/70], CNN Loss: 0.9058, CNN Accuracy: 68.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 32/70 [02:29<03:16,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/70], CNN Loss: 0.8444, CNN Accuracy: 71.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 33/70 [02:33<03:06,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/70], CNN Loss: 0.8106, CNN Accuracy: 70.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 34/70 [02:38<03:00,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/70], CNN Loss: 0.7928, CNN Accuracy: 72.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 35/70 [02:43<02:52,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/70], CNN Loss: 0.7532, CNN Accuracy: 74.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 36/70 [02:48<02:48,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/70], CNN Loss: 0.6444, CNN Accuracy: 77.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 37/70 [02:54<02:48,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/70], CNN Loss: 0.6072, CNN Accuracy: 80.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 38/70 [02:59<02:44,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/70], CNN Loss: 0.5846, CNN Accuracy: 80.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 39/70 [03:05<02:45,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/70], CNN Loss: 0.5101, CNN Accuracy: 82.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 40/70 [03:10<02:44,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/70], CNN Loss: 0.4993, CNN Accuracy: 82.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 41/70 [03:15<02:35,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/70], CNN Loss: 0.4474, CNN Accuracy: 85.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 42/70 [03:20<02:25,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/70], CNN Loss: 0.4276, CNN Accuracy: 85.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 43/70 [03:25<02:17,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/70], CNN Loss: 0.3693, CNN Accuracy: 87.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 44/70 [03:30<02:11,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/70], CNN Loss: 0.3278, CNN Accuracy: 88.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 45/70 [03:35<02:05,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/70], CNN Loss: 0.3450, CNN Accuracy: 88.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 46/70 [03:40<02:01,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/70], CNN Loss: 0.3159, CNN Accuracy: 88.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 47/70 [03:45<01:55,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/70], CNN Loss: 0.2815, CNN Accuracy: 90.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 48/70 [03:50<01:50,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/70], CNN Loss: 0.2399, CNN Accuracy: 91.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 49/70 [03:55<01:45,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/70], CNN Loss: 0.2294, CNN Accuracy: 91.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 50/70 [04:00<01:40,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/70], CNN Loss: 0.2128, CNN Accuracy: 92.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 51/70 [04:06<01:37,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/70], CNN Loss: 0.2047, CNN Accuracy: 92.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 52/70 [04:11<01:34,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/70], CNN Loss: 0.1902, CNN Accuracy: 93.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 53/70 [04:17<01:33,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/70], CNN Loss: 0.1650, CNN Accuracy: 94.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 54/70 [04:24<01:32,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/70], CNN Loss: 0.1673, CNN Accuracy: 94.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 55/70 [04:31<01:31,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/70], CNN Loss: 0.1245, CNN Accuracy: 95.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 56/70 [04:36<01:23,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/70], CNN Loss: 0.1360, CNN Accuracy: 95.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 57/70 [04:41<01:14,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/70], CNN Loss: 0.1411, CNN Accuracy: 95.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 58/70 [04:47<01:09,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/70], CNN Loss: 0.1232, CNN Accuracy: 96.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 59/70 [04:53<01:01,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/70], CNN Loss: 0.1103, CNN Accuracy: 96.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 60/70 [04:59<00:58,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/70], CNN Loss: 0.1157, CNN Accuracy: 95.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 61/70 [05:04<00:52,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/70], CNN Loss: 0.1436, CNN Accuracy: 95.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 62/70 [05:10<00:45,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/70], CNN Loss: 0.0949, CNN Accuracy: 96.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 63/70 [05:16<00:40,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/70], CNN Loss: 0.1210, CNN Accuracy: 95.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 64/70 [05:22<00:35,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/70], CNN Loss: 0.1092, CNN Accuracy: 96.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 65/70 [05:28<00:28,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/70], CNN Loss: 0.1122, CNN Accuracy: 96.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 66/70 [05:33<00:22,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/70], CNN Loss: 0.1187, CNN Accuracy: 95.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 67/70 [05:39<00:17,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/70], CNN Loss: 0.1041, CNN Accuracy: 97.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 68/70 [05:45<00:11,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/70], CNN Loss: 0.0849, CNN Accuracy: 96.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 69/70 [05:52<00:06,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/70], CNN Loss: 0.0871, CNN Accuracy: 97.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [05:58<00:00,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/70], CNN Loss: 0.1045, CNN Accuracy: 96.50%\n",
      "Training loop ended...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 70\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    outputs = model(X_train)\n",
    "    outputs = outputs.squeeze(1)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    _, predicted_labels = torch.max(outputs, 1)\n",
    "    accuracy = accuracy_score(y_train.numpy().flatten(), predicted_labels.numpy())\n",
    "\n",
    "    '''model.eval()\n",
    "    test_outputs = model(X_test)\n",
    "    test_outputs = test_outputs.squeeze(1)\n",
    "    y_test = y_test.view(-1)\n",
    "\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "    _, predicted_test_labels = torch.max(test_outputs, 1)\n",
    "    test_accuracy = accuracy_score(y_test.numpy().flatten(), predicted_test_labels.numpy())\n",
    "\n",
    "    print(f'Test Loss: {test_loss.item():.4f}, Test Accuracy: {test_accuracy:.2%}')'''\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], CNN Loss: {loss.item():.4f}, CNN Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "print(\"Training loop ended...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8775, Test Accuracy: 67.20%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_outputs = model(X_test)\n",
    "    test_outputs = test_outputs.squeeze(1)\n",
    "    y_test = y_test.view(-1)\n",
    "\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "    _, predicted_test_labels = torch.max(test_outputs, 1)\n",
    "    test_accuracy = accuracy_score(y_test.numpy().flatten(), predicted_test_labels.numpy())\n",
    "\n",
    "    print(f'Test Loss: {test_loss.item():.4f}, Test Accuracy: {test_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Completed!\n",
      "tensor([5, 4, 4, 9, 6, 2, 6, 9, 1, 9, 1, 0, 3, 4, 2, 3, 5, 6, 1, 4, 8, 2, 8, 9,\n",
      "        0, 3, 5, 4, 5, 2, 5, 0, 4, 8, 3, 4, 7, 5, 7, 5, 9, 5, 7, 5, 3, 6, 5, 0,\n",
      "        7, 9, 1, 7, 2, 0, 0, 0, 4, 5, 7, 5, 9, 5, 1, 8, 0, 4, 5, 0, 2, 1, 2, 9,\n",
      "        9, 4, 5, 4, 5, 9, 5, 0, 7, 1, 8, 0, 5, 7, 8, 1, 7, 7, 7, 2, 0, 3, 2, 5,\n",
      "        4, 6, 4, 0, 8, 9, 8, 7, 2, 4, 1, 3, 5, 2, 5, 9, 5, 8, 3, 3, 6, 6, 7, 7,\n",
      "        1, 0, 0, 7, 8, 1, 3, 8, 6, 1, 1, 6, 8, 7, 4, 1, 4, 2, 0, 1, 7, 4, 4, 5,\n",
      "        4, 4, 5, 0, 2, 8, 1, 5, 2, 3, 3, 7, 7, 6, 0, 7, 5, 7, 2, 1, 0, 9, 3, 9,\n",
      "        8, 6, 8, 3, 2, 5, 3, 3, 3, 1, 7, 2, 6, 1, 1, 5, 2, 5, 9, 2, 1, 5, 9, 4,\n",
      "        3, 5, 4, 5, 2, 2, 2, 4, 5, 1, 9, 4, 3, 6, 1, 4, 4, 0, 8, 0, 9, 7, 7, 4,\n",
      "        7, 7, 8, 2, 6, 9, 7, 9, 8, 7, 6, 5, 7, 6, 8, 6, 7, 2, 2, 3, 1, 5, 9, 9,\n",
      "        3, 7, 4, 7, 3, 3, 0, 6, 3, 3, 4, 5, 7, 4, 1, 7, 7, 5, 0, 9, 5, 1, 7, 0,\n",
      "        6, 6, 5, 3, 5, 9, 0, 1, 9, 4, 1, 6, 4, 1, 6, 0, 1, 7, 7, 3, 3, 7, 0, 5,\n",
      "        7, 7, 2, 9, 8, 6, 9, 4, 8, 7, 6, 6, 6, 3, 5, 1, 0, 8, 3, 3, 5, 6, 6, 7,\n",
      "        1, 6, 8, 0, 0, 3, 3, 1, 2, 6, 2, 8, 7, 9, 5, 0, 1, 8, 8, 6, 0, 1, 6, 5,\n",
      "        5, 3, 2, 4, 1, 2, 6, 4, 1, 5, 1, 4, 9, 4, 8, 1, 6, 8, 3, 4, 9, 1, 8, 7,\n",
      "        3, 5, 7, 7, 0, 4, 9, 4, 4, 7, 6, 8, 8, 7, 0, 1, 4, 5, 7, 3, 2, 2, 4, 0,\n",
      "        0, 8, 5, 9, 0, 5, 6, 3, 8, 0, 2, 7, 9, 6, 2, 0, 9, 7, 6, 7, 6, 5, 2, 3,\n",
      "        9, 5, 1, 6, 9, 9, 7, 7, 1, 9, 2, 8, 2, 0, 2, 0, 8, 2, 8, 0, 4, 1, 9, 8,\n",
      "        4, 5, 9, 0, 1, 9, 5, 0, 7, 5, 8, 0, 8, 7, 6, 3, 7, 0, 7, 2, 6, 1, 8, 5,\n",
      "        6, 2, 1, 5, 7, 1, 6, 4, 4, 8, 8, 2, 8, 1, 8, 8, 9, 3, 6, 7, 1, 7, 9, 3,\n",
      "        5, 1, 7, 9, 1, 3, 3, 4, 6, 9, 0, 9, 6, 2, 5, 9, 6, 2, 0, 8])\n",
      "tensor([5, 0, 2, 9, 0, 2, 6, 9, 4, 9, 7, 0, 3, 3, 2, 3, 7, 6, 1, 4, 8, 2, 8, 9,\n",
      "        0, 3, 5, 0, 5, 1, 5, 0, 3, 8, 3, 4, 7, 5, 7, 5, 9, 5, 9, 5, 3, 6, 5, 0,\n",
      "        7, 9, 3, 3, 2, 0, 8, 0, 4, 5, 8, 5, 9, 5, 1, 8, 1, 4, 5, 1, 8, 1, 2, 9,\n",
      "        9, 1, 9, 4, 5, 9, 5, 9, 7, 7, 8, 0, 4, 0, 8, 1, 7, 1, 3, 2, 0, 9, 3, 5,\n",
      "        4, 6, 3, 0, 7, 9, 8, 7, 1, 4, 1, 3, 5, 2, 5, 2, 7, 9, 3, 3, 6, 6, 7, 7,\n",
      "        1, 0, 0, 7, 8, 1, 2, 8, 6, 1, 5, 4, 8, 7, 7, 1, 4, 2, 0, 1, 7, 7, 4, 5,\n",
      "        4, 3, 5, 0, 3, 8, 7, 5, 4, 3, 7, 7, 7, 6, 0, 7, 5, 7, 2, 1, 0, 9, 3, 9,\n",
      "        8, 0, 3, 3, 9, 3, 3, 2, 3, 2, 7, 2, 6, 2, 8, 5, 2, 5, 9, 5, 8, 1, 4, 3,\n",
      "        9, 3, 4, 7, 2, 8, 8, 3, 5, 1, 9, 4, 5, 6, 1, 3, 9, 0, 3, 0, 9, 5, 7, 4,\n",
      "        3, 7, 1, 1, 5, 9, 7, 9, 8, 7, 5, 5, 7, 2, 7, 6, 8, 2, 2, 3, 0, 4, 9, 9,\n",
      "        3, 9, 6, 4, 3, 9, 1, 6, 9, 3, 2, 5, 7, 9, 6, 5, 7, 9, 8, 9, 5, 1, 7, 0,\n",
      "        6, 6, 5, 3, 0, 9, 0, 1, 6, 4, 1, 0, 6, 7, 3, 1, 8, 7, 9, 3, 3, 9, 0, 3,\n",
      "        5, 7, 3, 9, 7, 6, 9, 4, 8, 7, 5, 6, 6, 3, 5, 7, 1, 8, 3, 9, 3, 6, 6, 9,\n",
      "        1, 6, 8, 0, 1, 3, 2, 1, 8, 6, 2, 8, 7, 9, 7, 0, 1, 8, 1, 6, 0, 1, 6, 5,\n",
      "        5, 3, 2, 4, 2, 2, 6, 1, 1, 5, 1, 4, 9, 4, 8, 1, 6, 7, 3, 2, 9, 1, 7, 7,\n",
      "        3, 6, 7, 7, 0, 4, 9, 9, 4, 7, 6, 8, 8, 7, 0, 1, 9, 3, 1, 3, 2, 2, 3, 0,\n",
      "        0, 8, 5, 6, 2, 9, 6, 3, 8, 0, 2, 7, 9, 6, 3, 8, 4, 5, 7, 7, 6, 5, 6, 2,\n",
      "        9, 0, 1, 6, 9, 9, 7, 7, 1, 9, 2, 0, 2, 1, 2, 0, 9, 1, 8, 8, 9, 1, 3, 8,\n",
      "        3, 5, 9, 0, 1, 9, 5, 0, 8, 5, 0, 8, 8, 7, 6, 3, 0, 0, 7, 2, 6, 1, 8, 7,\n",
      "        6, 2, 3, 5, 5, 1, 4, 4, 4, 7, 9, 2, 8, 2, 1, 3, 9, 3, 9, 7, 1, 4, 9, 3,\n",
      "        5, 8, 2, 9, 1, 3, 3, 4, 6, 9, 0, 9, 6, 2, 5, 9, 6, 8, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Evaluation Completed!\")\n",
    "print(y_test)\n",
    "print(predicted_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_inversed_encoded = label_encoder.inverse_transform(y_test)\n",
    "predicted_test_labels_inversed_encoded = label_encoder.inverse_transform(predicted_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as \n",
      "\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "#model_path = os.path.join(current_path, \"saved_model.pt\")\n",
    "torch.save(model.state_dict(), \"C:/Users/lly/Desktop/CUHK/Year 4/Sem 1/AIST2010/Project/Singers/saved_model.pt\")\n",
    "print(f\"Model saved as \\n\") #{model_path}\n",
    "\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "singer_accuracy = {}\n",
    "for singer in label_encoder.classes_:\n",
    "    singer_indices = np.where(y_test_inversed_encoded == singer)[0]\n",
    "    singer_true_labels = y_test_inversed_encoded[singer_indices]\n",
    "    singer_predicted_labels = predicted_test_labels_inversed_encoded[singer_indices]\n",
    "    singer_acc = accuracy_score(singer_true_labels, singer_predicted_labels)\n",
    "    singer_accuracy[singer] = singer_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the singers based on accuracy in descending order\n",
    "sorted_singer_accuracy = sorted(singer_accuracy.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singer Accuracies:\n",
      "The Weeknd: 87.23%\n",
      "Billie Eilish: 75.00%\n",
      "Justin Bieber: 75.00%\n",
      "Adele: 68.75%\n",
      "Ed Sheeran: 68.33%\n",
      "Lady Gaga: 63.49%\n",
      "Ariana Grande: 63.46%\n",
      "Taylor Swift: 63.04%\n",
      "Beyonce: 61.36%\n",
      "Dua Lipa: 47.92%\n"
     ]
    }
   ],
   "source": [
    "# Print the singers and their accuracies\n",
    "print(\"Singer Accuracies:\")\n",
    "for singer, acc in sorted_singer_accuracy:\n",
    "    print(f\"{singer}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def preprocess_test(audio_data):\n",
    "    # Convert audio to mono\n",
    "    audio_data = librosa.to_mono(audio_data)\n",
    "\n",
    "    # Normalize audio\n",
    "    audio_data = librosa.util.normalize(audio_data)\n",
    "\n",
    "    return audio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the audio file\n",
    "audio_file = \"C:/Users/lly/Desktop/CUHK/Year 4/Sem 1/AIST2010/Project/Singers/Adele/Hello_Adele.wav\"  # Specify the path to your audio file\n",
    "\n",
    "audio_data, sr = librosa.load(audio_file, sr=22050)  # Read the audio file\n",
    "\n",
    "normalized_audio = preprocess_test(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the audio into 1-second chunks\n",
    "segment_duration = 1  # Duration of each segment in seconds\n",
    "segment_length = int(segment_duration * sr)\n",
    "num_segments = len(normalized_audio) // segment_length\n",
    "segments = np.array_split(normalized_audio[:num_segments * segment_length], num_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_features = []\n",
    "for segment in segments:\n",
    "    mfcc_segment = mfcc(segment)\n",
    "    mfcc_features.append(mfcc_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "mfcc_features = np.array(mfcc_features)\n",
    "\n",
    "# Standardize the MFCC features\n",
    "scaler = StandardScaler()\n",
    "mfcc_features = scaler.fit_transform(mfcc_features)\n",
    "\n",
    "mfcc_tensor = torch.tensor(mfcc_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(mfcc_tensor.unsqueeze(1)) \n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "    #outputs = outputs.squeeze(1)\n",
    "    #_, predicted_labels = torch.max(outputs, 1)\n",
    "    predicted_labels = torch.argmax(probabilities, dim=1)\n",
    "    predicted_singers = label_encoder.inverse_transform(predicted_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1569e-01, 5.0080e-01, 1.6672e-02,  ..., 5.1729e-02, 4.9037e-03,\n",
       "         2.8663e-05],\n",
       "        [2.3798e-12, 2.2204e-07, 5.1935e-01,  ..., 2.8656e-07, 1.0202e-15,\n",
       "         1.2227e-06],\n",
       "        [2.6493e-07, 4.1107e-03, 4.8497e-02,  ..., 7.5780e-04, 2.2883e-04,\n",
       "         4.0406e-06],\n",
       "        ...,\n",
       "        [1.9574e-10, 8.2881e-19, 2.6104e-12,  ..., 1.0332e-10, 2.5189e-06,\n",
       "         1.4987e-14],\n",
       "        [7.9626e-01, 8.5403e-02, 5.4124e-07,  ..., 1.1833e-01, 1.2302e-07,\n",
       "         2.7325e-19],\n",
       "        [1.1015e-01, 8.0866e-05, 7.0667e-05,  ..., 7.5428e-01, 1.2210e-02,\n",
       "         1.1297e-11]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_probability_index = torch.argmax(probabilities.mean(dim=0))\n",
    "highest_probability_singer = label_encoder.classes_[highest_probability_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Singers: ['Ariana Grande' 'Beyonce' 'Billie Eilish' 'Ariana Grande' 'Billie Eilish'\n",
      " 'Beyonce' 'Ed Sheeran' 'Adele' 'Adele' 'Ed Sheeran' 'Ariana Grande'\n",
      " 'Ed Sheeran' 'Beyonce' 'Beyonce' 'Beyonce' 'Ed Sheeran' 'Billie Eilish'\n",
      " 'Beyonce' 'Beyonce' 'Ariana Grande' 'Lady Gaga' 'Lady Gaga'\n",
      " 'Billie Eilish' 'Ed Sheeran' 'Beyonce' 'Adele' 'Ed Sheeran'\n",
      " 'Ariana Grande' 'Ed Sheeran' 'Justin Bieber' 'Lady Gaga' 'Adele'\n",
      " 'Lady Gaga' 'Adele' 'Ed Sheeran' 'Beyonce' 'Dua Lipa' 'Ed Sheeran'\n",
      " 'Billie Eilish' 'Lady Gaga' 'Billie Eilish' 'Billie Eilish' 'Lady Gaga'\n",
      " 'Lady Gaga' 'Ariana Grande' 'Dua Lipa' 'Ed Sheeran' 'Adele' 'Beyonce'\n",
      " 'Ariana Grande' 'Ariana Grande' 'Dua Lipa' 'Ed Sheeran' 'The Weeknd'\n",
      " 'Billie Eilish' 'Ed Sheeran' 'Lady Gaga' 'Lady Gaga' 'Ed Sheeran'\n",
      " 'Beyonce' 'Billie Eilish' 'Beyonce' 'Taylor Swift' 'Adele' 'Beyonce'\n",
      " 'Beyonce' 'Taylor Swift' 'Ariana Grande' 'Lady Gaga' 'Adele'\n",
      " 'Taylor Swift' 'Beyonce' 'Taylor Swift' 'Taylor Swift' 'Lady Gaga'\n",
      " 'Taylor Swift' 'Lady Gaga' 'Adele' 'The Weeknd' 'Lady Gaga' 'The Weeknd'\n",
      " 'Lady Gaga' 'Lady Gaga' 'The Weeknd' 'Beyonce' 'Beyonce' 'Taylor Swift'\n",
      " 'Taylor Swift' 'Beyonce' 'Taylor Swift' 'Taylor Swift' 'Taylor Swift'\n",
      " 'Lady Gaga' 'Ed Sheeran' 'Taylor Swift' 'Taylor Swift' 'Beyonce' 'Adele'\n",
      " 'Taylor Swift' 'Beyonce' 'Lady Gaga' 'Lady Gaga' 'Justin Bieber' 'Adele'\n",
      " 'Lady Gaga' 'Ariana Grande' 'Ariana Grande' 'Adele' 'Justin Bieber'\n",
      " 'Justin Bieber' 'Taylor Swift' 'Taylor Swift' 'Taylor Swift'\n",
      " 'Billie Eilish' 'Billie Eilish' 'Lady Gaga' 'Beyonce' 'Beyonce'\n",
      " 'Lady Gaga' 'Adele' 'The Weeknd' 'Ariana Grande' 'Adele' 'The Weeknd'\n",
      " 'Ariana Grande' 'Lady Gaga' 'Dua Lipa' 'Beyonce' 'Ed Sheeran' 'Adele'\n",
      " 'Adele' 'Ed Sheeran' 'Beyonce' 'Ariana Grande' 'Ed Sheeran' 'Lady Gaga'\n",
      " 'Adele' 'The Weeknd' 'Adele' 'Justin Bieber' 'Taylor Swift' 'Dua Lipa'\n",
      " 'Ariana Grande' 'Beyonce' 'Dua Lipa' 'Dua Lipa' 'Billie Eilish' 'Beyonce'\n",
      " 'Adele' 'Lady Gaga' 'Billie Eilish' 'Billie Eilish' 'Billie Eilish'\n",
      " 'Lady Gaga' 'Taylor Swift' 'Beyonce' 'Taylor Swift' 'Taylor Swift'\n",
      " 'Taylor Swift' 'The Weeknd' 'Billie Eilish' 'Taylor Swift' 'Taylor Swift'\n",
      " 'Taylor Swift' 'Adele' 'The Weeknd' 'Taylor Swift' 'Lady Gaga'\n",
      " 'Lady Gaga' 'The Weeknd' 'The Weeknd' 'Dua Lipa' 'Lady Gaga'\n",
      " 'Taylor Swift' 'Justin Bieber' 'Justin Bieber' 'The Weeknd' 'The Weeknd'\n",
      " 'Ed Sheeran' 'Ed Sheeran' 'Beyonce' 'Billie Eilish' 'Justin Bieber'\n",
      " 'Adele' 'Adele' 'Taylor Swift' 'Beyonce' 'Taylor Swift' 'Adele'\n",
      " 'Billie Eilish' 'Dua Lipa' 'The Weeknd' 'Lady Gaga' 'The Weeknd'\n",
      " 'Beyonce' 'Taylor Swift' 'Justin Bieber' 'Justin Bieber' 'Adele'\n",
      " 'Dua Lipa' 'Adele' 'Ariana Grande' 'Billie Eilish' 'Billie Eilish'\n",
      " 'The Weeknd' 'The Weeknd' 'The Weeknd' 'The Weeknd' 'Dua Lipa'\n",
      " 'Justin Bieber' 'The Weeknd' 'The Weeknd' 'The Weeknd' 'Ed Sheeran'\n",
      " 'Dua Lipa' 'The Weeknd' 'The Weeknd' 'The Weeknd' 'The Weeknd'\n",
      " 'Taylor Swift' 'Ariana Grande' 'Ed Sheeran' 'Taylor Swift'\n",
      " 'Ariana Grande' 'Ariana Grande' 'Billie Eilish' 'The Weeknd' 'Ed Sheeran'\n",
      " 'Justin Bieber' 'Taylor Swift' 'Justin Bieber' 'Lady Gaga' 'Dua Lipa'\n",
      " 'Justin Bieber' 'Justin Bieber' 'Taylor Swift' 'Taylor Swift'\n",
      " 'The Weeknd' 'The Weeknd' 'The Weeknd' 'Justin Bieber' 'The Weeknd'\n",
      " 'The Weeknd' 'The Weeknd' 'The Weeknd' 'Ed Sheeran' 'Justin Bieber'\n",
      " 'The Weeknd' 'Justin Bieber' 'The Weeknd' 'The Weeknd' 'The Weeknd'\n",
      " 'Justin Bieber' 'Justin Bieber' 'Taylor Swift' 'Beyonce' 'Ed Sheeran'\n",
      " 'Adele' 'Justin Bieber' 'Ed Sheeran' 'Justin Bieber' 'Adele'\n",
      " 'Billie Eilish' 'Ed Sheeran' 'Adele' 'Justin Bieber' 'The Weeknd'\n",
      " 'Beyonce' 'Lady Gaga' 'Billie Eilish' 'Ed Sheeran' 'Adele' 'The Weeknd'\n",
      " 'Ed Sheeran' 'Ed Sheeran' 'Adele' 'Lady Gaga']\n",
      "Singer with Highest Probability: The Weeknd\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Singers:\", predicted_singers)\n",
    "print(\"Singer with Highest Probability:\", highest_probability_singer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
